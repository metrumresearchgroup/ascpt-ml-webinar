---
title: "Preliminary Modeling"
author: "Matthew Wiens"
output: pdf_document
date: '2023-05-24'
editor_options: 
  chunk_output_type: console
---

# Setup

Load libraries and set up file paths.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(tidymodels)
library(xgboost)

data_dir <- here::here("data", "source")
```

Data source: https://archive.ics.uci.edu/ml/datasets/breast+cancer

```{R}

# Pull names from the data description file, since the csv does not have column names
input_column_names <- c("Class", "age", "menopause", "tumor_size", "inv_nodes", "node_caps", "deg_malig", "breast", "breast_quad", "irradiat")

# Load data, with options for this specific dataset 
dat_raw <- read_csv(file = file.path(data_dir, "breast-cancer.data"),
         col_names = input_column_names,
         na = c("", "NA", "?")) 
  

```

Data description:

   1. Class: no-recurrence-events, recurrence-events
   2. age: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, 90-99.
   3. menopause: lt40, ge40, premeno.
   4. tumor-size: 0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44,
                  45-49, 50-54, 55-59.
   5. inv-nodes: 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23, 24-26,
                 27-29, 30-32, 33-35, 36-39.
   6. node-caps: yes, no.
   7. deg-malig: 1, 2, 3.
   8. breast: left, right.
   9. breast-quad: left-up, left-low, right-up, right-low, central.
  10. irradiat: yes, no.
  
  
Cleanup the ordered categories to be numeric here  
```{R}

# Further preprocessing, using tidyverse
# Trees naturally handle ordered factors (categorical) covariates,
# so we code the data in that way
# The importance_weights column will be used later in the script
dat <- dat_raw %>%
  mutate(class_n = Class == "recurrence-events",
         Class = factor(Class, levels = c("recurrence-events", "no-recurrence-events")),
         age_f = factor(age, ordered = T),
         tumor_size_f = factor(tumor_size,
                               ordered = T, 
                               levels = c("0-4", "5-9", "10-14", "15-19",
                                          "20-24", "25-29", "30-34", "35-39", 
                                          "40-44", "45-49",  "50-54")),
         inv_nodes_f = factor(inv_nodes, 
                              ordered = T,
                              levels = c("0-2", "3-5", "6-8", "9-11", "12-14", "15-17", "24-26")),
         irradiat_n = irradiat == "yes",
         node_caps_n = node_caps == "yes",
         breast_left = breast == "left",
         case_weight = importance_weights(if_else(class_n == 0, 1, 201/85))
  )

# Create the training/testing split with the tidymodels package
init_split <- initial_split(dat)
```


Create the formatted datasets for training and testing in a format for xgboost, which is a matrix. 
Also, the recipes package is used to preprocess categorical and ordinal covariates into numerics for xgboost.

```{R}

# This is like specifying a model, but for data pre-proecessing 
recipe_xgboost <- recipes::recipe(class_n ~ age_f + menopause + tumor_size_f + inv_nodes_f + node_caps_n + deg_malig + breast_left + breast_quad + irradiat_n, 
                data = training(init_split)) %>%
  step_ordinalscore(age_f, tumor_size_f, inv_nodes_f) %>%
  step_dummy(breast_quad, menopause, one_hot = T)

# This is like fitting the model 
estimated_preprocessing <- prep(recipe_xgboost, training(init_split))

# This is like using the model to make predictions
x_train <- bake(estimated_preprocessing, training(init_split)) %>% select(-class_n) %>% as.matrix()
y_train <- bake(estimated_preprocessing, training(init_split)) %>% pull(class_n) 

x_test <- bake(estimated_preprocessing, testing(init_split)) %>% select(-class_n) %>% as.matrix()
y_test <- bake(estimated_preprocessing, testing(init_split)) %>% pull(class_n) 

```

Fit the model with default tuning parameters. 

```{R}

fit1 <- xgboost::xgboost(data = x_train, 
                         label = y_train,
                         nrounds = 15,
                         params = list(
                           objective = "binary:logistic"
                         ))

```

Evaluate fit

```{R}

# Using the test set here, which was not used to fit the model
predictions <- testing(init_split) %>%
  mutate(pred_pr = predict(fit1, x_test))

# ROC curves are a common way to assess a binary prediction
predictions %>%
  mutate(truth = Class) %>%
  roc_auc(pred_pr, truth = truth, estimator = "binary", event_level = "first")

# Confusion matrix for predictions
predictions %>%
  mutate(truth = Class,
         pred = factor(if_else(pred_pr >= 0.5, "no-recurrence-events", "recurrence-events", ),  levels = c("recurrence-events", "no-recurrence-events"))) %>%
  yardstick::conf_mat(estimate = pred, truth = truth)

# The yardstick package has many functions to evaluate (and plot evaluation) of models, and works in the tidymodels package

```


# Hyperparameter Tuning and Tidymodels


Source: https://juliasilge.com/blog/xgboost-tune-volleyball/
```{R}

# Slightly tweak the formatting of the dataset for tidymodels format
# Also, for simplification of code, we'll just impute missing values
# Even though in the previous example xgboost cleanly handles them 
recipe_xgboost_tidymodels <- recipes::recipe(
  Class ~age_f + menopause + tumor_size_f + inv_nodes_f + node_caps_n + 
    deg_malig + breast_left + breast_quad + irradiat_n, 
                data = training(init_split)) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  step_impute_mean(all_numeric_predictors()) %>%
  step_ordinalscore(age_f, tumor_size_f, inv_nodes_f) %>%
  step_mutate(node_caps_n = as.numeric(node_caps_n), 
              breast_left = as.numeric(breast_left), 
              irradiat_n = as.numeric(irradiat_n)) %>%
  step_dummy(breast_quad, menopause, one_hot = T) 
  
# Using the recipes again
estimated_preprocessing2 <- prep(recipe_xgboost_tidymodels, training(init_split))

bake(estimated_preprocessing2, training(init_split))

# Everything that could be tuned, within reason
# But this isn't the most practical approach (curse of dimensionality)
# Also, note the tuning parameter names are slightly different in tidymodels
# compared to xgboost
xgb_spec_all <- boost_tree(
  trees = 1000,   # Number of trees in ensemble
  tree_depth = tune(),      # Max depth of the tree (number of splits) 
  min_n = tune(),    # Min number of data points in a node that is required for the node to be split further
  loss_reduction = tune(),    # Number for reduction in loss function required to split further    
  sample_size = tune(), # Proportion of data exposed to fitting routine
  mtry = tune(),     # Proportion of predictors randomly sampled at each split when creating tree models    
  learn_rate = tune()      # Step size
) %>%
  set_engine("xgboost") %>%
  set_mode("classification")


# Here's a more limited set of parameters to tune
xgb_spec <- boost_tree(
  trees = 15,
  tree_depth = tune(),
  min_n = tune(),
  loss_reduction = 0,        
  sample_size = 1.0, 
  learn_rate = 0.3      
) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

# One way to select the points to tune
# See the help/google for more details
# Fitting 50 points for tree_depth and min_n in a random grid for hyperparameter tuning
xgb_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  size = 50 
)

# Combine preprocessing and fitting into a "workflow"
xgb_wf <- workflow() %>%
  add_recipe(recipe_xgboost_tidymodels) %>%
  add_model(xgb_spec)

# Set up cross-validation  
cv_folds <- vfold_cv(training(init_split), v = 5)

set.seed(1234) # setting random seed to eliminate stochastic behavior of model

# Question: How many models do we have to fit?

# Run the cross-validation
tuning_results <- tune_grid(
  xgb_wf,
  resamples = cv_folds,
  grid = xgb_grid,
  control = control_grid(save_pred = TRUE, verbose = TRUE,  event_level = "first")
)

# Summary of the results
collect_metrics(tuning_results)

# Analysis of the results
# ROC AUC
best_auc <- select_best(tuning_results, "roc_auc")

# Create the final model, with actual values for the hyperparameters 
final_wf <- finalize_workflow(
  xgb_wf,
  best_auc
)

# Some ways to examine the final fit
final_fit <- final_wf %>%
  last_fit(init_split) 

final_fit %>%
  collect_predictions() %>% 
  roc_curve(Class, `.pred_recurrence-events`, event_level = "first") %>% 
  autoplot()

final_fit %>%
    collect_predictions() %>% 
    roc_auc(Class, `.pred_recurrence-events`, event_level = "first")

final_fit %>%
    collect_predictions() %>% 
    accuracy(Class, .pred_class)

# Question: Is this accuracy good for this dataset? What accuracy would a very naive algorithm have? 

# Answer: Well a model always predicting no events would have approx 70% accuracy

# Question: What about focusing on the patients with recurrence-events? What are alternatives to ROC in the yardstick package for calculating an AUC?

final_fit %>%
  collect_predictions() %>% 
  pr_curve(Class, `.pred_recurrence-events`, event_level = "first") %>% 
  autoplot()

final_fit %>%
    collect_predictions() %>% 
    pr_auc(Class, `.pred_recurrence-events`, event_level = "first")

# We can look at the sequence of trees fit by xgboost for the final model
# Here's the first one
# This is output in the Rstudio viewer
xgboost::xgb.plot.tree(model = extract_fit_parsnip(final_fit)$fit, trees = 0)
```

## Also, something about imbalanced classes

```{R}

set.seed(12345)

recipe_xgboost_tidymodels_weights <- recipes::recipe(
  Class ~age_f + menopause + tumor_size_f + inv_nodes_f + node_caps_n + 
    deg_malig + breast_left + breast_quad + irradiat_n + case_weight, 
                data = training(init_split)) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  step_impute_mean(all_numeric_predictors()) %>%
  step_ordinalscore(age_f, tumor_size_f, inv_nodes_f) %>%
  step_mutate(node_caps_n = as.numeric(node_caps_n), 
              breast_left = as.numeric(breast_left), 
              irradiat_n = as.numeric(irradiat_n)) %>%
  step_dummy(breast_quad, menopause, one_hot = T) 

xgb_wf_weights <- xgb_wf %>%
  update_recipe(recipe_xgboost_tidymodels_weights) %>%
  add_case_weights(case_weight)

tuning_results_weights <- tune_grid(
  xgb_wf_weights,
  resamples = cv_folds,
  grid = xgb_grid,
  control = control_grid(save_pred = TRUE, verbose = TRUE,  event_level = "first")
)

collect_metrics(tuning_results_weights)

best_auc_weighted <- select_best(tuning_results_weights, "pr_auc")


# Create the final model, with actual values for the hyperparameters 
final_wf_weighted <- finalize_workflow(
  xgb_wf_weights,
  best_auc_weighted
)

final_fit_weighted <- final_wf_weighted %>%
  last_fit(init_split) 

final_fit_weighted %>%
  collect_predictions() %>% 
  pr_curve(Class, `.pred_recurrence-events`, event_level = "first") %>% 
  autoplot()

final_fit_weighted %>%
    collect_predictions() %>% 
    pr_auc(Class, `.pred_recurrence-events`, event_level = "first")


final_fit_weighted %>%
    collect_predictions() %>% 
    roc_auc(Class, `.pred_recurrence-events`, event_level = "first")
```


## Model interpretation with Shapley Values


```{R}

# Note the shapley values are for predicting the first class in the Class factor, which is no recurrence events
# Always good to pay attention to how R and libraries are using factor orderings
shap_plot <- xgb.ggplot.shap.summary(data = bake(extract_recipe(final_fit), testing(init_split)) %>% select(-Class) %>% as.matrix(),
                        model = extract_fit_parsnip(final_fit)$fit) +
  labs(x = "Shapley Value", color = "Feature Value") 

shap_plot



```

Sanity check the shapley predictions by making predictions for two different values of tumor sizes for one set of covariate values

```{R}

# Create a dataset of covariates to make predictions
x_predict <- testing(init_split)[1,] %>%
  select(-tumor_size_f) %>%
  dplyr::inner_join(tibble(tumor_size_f = c("0-4", "50-54")), by = character())


# Make the predictions
predict(extract_workflow(final_fit),
        new_data = x_predict,
        type = "prob")
```
